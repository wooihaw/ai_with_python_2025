{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wooihaw/ai_with_python_2025/blob/main/handson_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBCmGpG1UPao"
      },
      "source": [
        "## Hands-on 8\n",
        "#### In this hands-on, an LLM will be installed and served.\n",
        "#### Retrieval Augmented Generation (RAG) will be used to to produce more accurate and contextually relevant outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uB3GaqzwmH72"
      },
      "outputs": [],
      "source": [
        "pip install requests beautifulsoup4 langchain langchain-community langchain-chroma langchain-ollama langchain-text-splitters colab-xterm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SR6YMNB-msY3"
      },
      "outputs": [],
      "source": [
        "!mkdir local_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imSoxCuYh-vY"
      },
      "outputs": [],
      "source": [
        "!sudo apt update && sudo apt install pciutils lshw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R04AkQT2hBDl"
      },
      "outputs": [],
      "source": [
        "!curl https://ollama.ai/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-YuIG1CmTfp"
      },
      "outputs": [],
      "source": [
        "!nohup ollama serve > ollama.log 2>&1 &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RG2MCzu8h9mR"
      },
      "outputs": [],
      "source": [
        "!ollama pull nomic-embed-text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTIqGMapjPsZ"
      },
      "outputs": [],
      "source": [
        "!ollama pull gemma3:1b-it-qat"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Without RAG\n",
        "\n",
        "import os\n",
        "# Import Ollama specific chat model\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "\n",
        "# Imports for Conversational Memory and LCEL\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "\n",
        "# --- Configuration ---\n",
        "# Ollama LLM model to use\n",
        "OLLAMA_LLM = \"gemma3:1b-it-qat\" # Or another LLM you have pulled (e.g., \"mistral\", \"gemma:2b\")\n",
        "\n",
        "# In-memory storage for chat history (per session)\n",
        "# This will reset every time the script is run.\n",
        "store = {} # Dictionary to hold chat histories keyed by session ID\n",
        "\n",
        "def get_session_history(session_id: str) -> ChatMessageHistory:\n",
        "    \"\"\"Returns the chat message history for a given session ID.\"\"\"\n",
        "    if session_id not in store:\n",
        "        store[session_id] = ChatMessageHistory()\n",
        "    return store[session_id]\n",
        "\n",
        "# --- Main Chat Implementation ---\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run a direct chat with Ollama LLM with memory.\n",
        "    \"\"\"\n",
        "    print(f\"Initializing Ollama LLM using model: {OLLAMA_LLM}\")\n",
        "    try:\n",
        "        # Initialize the ChatOllama model\n",
        "        ollama_llm = ChatOllama(\n",
        "            model=OLLAMA_LLM,\n",
        "            temperature=0.7 # A slightly higher temperature for more engaging chat,\n",
        "                            # but you can set to 0 for very deterministic answers.\n",
        "            # You might need to add base_url if Ollama is not on default host/port\n",
        "            # base_url=\"http://localhost:11434\"\n",
        "        )\n",
        "        # Quick test to ensure LLM is accessible\n",
        "        # response = ollama_llm.invoke(\"Hello!\")\n",
        "        # print(f\"LLM test response snippet: {response.content[:50]}...\")\n",
        "        print(\"Ollama LLM initialized successfully.\")\n",
        "        print(f\"Ensure Ollama server is running and model '{OLLAMA_LLM}' is pulled.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing Ollama LLM. Make sure Ollama server is running\")\n",
        "        print(f\"and model '{OLLAMA_LLM}' is pulled.\")\n",
        "        print(e)\n",
        "        return\n",
        "\n",
        "    # --- Setup Conversational Chain ---\n",
        "    # Define the prompt for the LLM\n",
        "    chat_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are a helpful and friendly AI assistant. Keep your responses concise.\"),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"), # Placeholder for the actual chat history\n",
        "        (\"user\", \"{input}\"), # The current user input\n",
        "    ])\n",
        "\n",
        "    # Define the core chat chain: Prompt -> LLM -> Output Parser -> Format for 'answer' key\n",
        "    # The output parser ensures the LLM's message content is a string\n",
        "    # RunnablePassthrough assigns this string to the 'answer' key in the final output dictionary.\n",
        "    core_chat_chain = chat_prompt | ollama_llm | StrOutputParser() | {\"answer\": RunnablePassthrough()}\n",
        "\n",
        "    # Wrap the core chat chain with RunnableWithMessageHistory to automatically manage chat history\n",
        "    with_message_history = RunnableWithMessageHistory(\n",
        "        core_chat_chain,\n",
        "        get_session_history,\n",
        "        input_messages_key=\"input\",          # Key for user input in the chain input dict\n",
        "        history_messages_key=\"chat_history\", # Key for chat history in the chain input dict\n",
        "        output_messages_key=\"answer\",        # Key for the AI's response in the chain output dict\n",
        "    )\n",
        "\n",
        "    print(\"\\nDirect chat with Ollama is ready!\")\n",
        "    print(\"Type 'quit' to end the chat.\")\n",
        "\n",
        "    # --- Chat Loop ---\n",
        "    session_id = \"single-user-session\" # Fixed session ID for this simple script\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nYour Message: \")\n",
        "        if user_input.lower() == 'quit':\n",
        "            print(\"Chat ended. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        if not user_input.strip():\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Invoke the wrapped chain. It automatically fetches history using session_id\n",
        "            # and updates history after getting the response.\n",
        "            response = with_message_history.invoke(\n",
        "                {\"input\": user_input},\n",
        "                config={\"configurable\": {\"session_id\": session_id}}\n",
        "            )\n",
        "\n",
        "            print(\"\\nAI:\", response['answer'])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "            print(\"Please ensure Ollama server is running and the model is available.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "njH413pdrYIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrK2mSvmkJIF"
      },
      "outputs": [],
      "source": [
        "# With RAG\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "# Import for loading local files\n",
        "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
        "\n",
        "# Import Ollama specific components\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Import for Conversational Memory and Chains\n",
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "\n",
        "# --- Configuration ---\n",
        "# List of URLs to scrape. Replace with your desired websites.\n",
        "SOURCE_URLS = [\n",
        "    # Add more URLs here\n",
        "]\n",
        "\n",
        "# Local Directory for Text Files\n",
        "# IMPORTANT: Create this directory and place your .txt files inside\n",
        "LOCAL_DOCS_DIR = \"./local_docs\"\n",
        "\n",
        "# Ollama models\n",
        "OLLAMA_LLM = \"gemma3:1b-it-qat\" # Or another LLM you have pulled\n",
        "OLLAMA_EMBEDDING_MODEL = \"nomic-embed-text\" # Or another embedding model you have pulled\n",
        "\n",
        "# Directory for persistent vector store\n",
        "CHROMA_DB_DIR = \"./chroma_db_ollama_memory\"\n",
        "\n",
        "# In-memory storage for chat history (per session)\n",
        "store = {}\n",
        "\n",
        "def get_session_history(session_id: str) -> ChatMessageHistory:\n",
        "    \"\"\"Returns the chat message history for a given session ID.\"\"\"\n",
        "    if session_id not in store:\n",
        "        store[session_id] = ChatMessageHistory()\n",
        "    return store[session_id]\n",
        "\n",
        "# --- Main RAG Implementation ---\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to perform RAG based on web and local text files using Ollama.\n",
        "    \"\"\"\n",
        "    # --- 1. Load Documents ---\n",
        "    print(\"Loading documents from URLs...\")\n",
        "    all_documents = []\n",
        "    # Load from URLs\n",
        "    for url in SOURCE_URLS:\n",
        "        try:\n",
        "            loader = WebBaseLoader(url)\n",
        "            docs = loader.load()\n",
        "            if docs:\n",
        "                print(f\"Loaded from web: {url}\")\n",
        "                all_documents.extend(docs)\n",
        "            else:\n",
        "                 print(f\"Could not load documents from: {url}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {url} using WebBaseLoader: {e}\")\n",
        "\n",
        "    # Load from local directory\n",
        "    print(f\"Loading documents from local directory: {LOCAL_DOCS_DIR}...\")\n",
        "    if os.path.exists(LOCAL_DOCS_DIR):\n",
        "        try:\n",
        "            # Use DirectoryLoader with TextLoader for .txt files\n",
        "            loader = DirectoryLoader(LOCAL_DOCS_DIR, glob=\"*.txt\", loader_cls=TextLoader)\n",
        "            local_docs = loader.load()\n",
        "            if local_docs:\n",
        "                print(f\"Loaded {len(local_docs)} documents from {LOCAL_DOCS_DIR}\")\n",
        "                all_documents.extend(local_docs)\n",
        "            else:\n",
        "                 print(f\"No .txt documents found in {LOCAL_DOCS_DIR}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading documents from local directory {LOCAL_DOCS_DIR}: {e}\")\n",
        "    else:\n",
        "        print(f\"Local documents directory not found: {LOCAL_DOCS_DIR}. Skipping local file loading.\")\n",
        "\n",
        "\n",
        "    if not all_documents:\n",
        "        print(\"No documents were loaded successfully from any source. Exiting.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Loaded {len(all_documents)} documents in total from all sources.\")\n",
        "\n",
        "    # --- 2. Process Documents (Splitting, Embedding, Vector Store) ---\n",
        "    # (This part processes the combined list of documents and remains the same)\n",
        "    print(\"Splitting documents into chunks...\")\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    split_documents = text_splitter.split_documents(all_documents)\n",
        "    print(f\"Split into {len(split_documents)} chunks.\")\n",
        "\n",
        "    print(\"Initializing Ollama Embeddings...\")\n",
        "    try:\n",
        "        embedding_function = OllamaEmbeddings(model=OLLAMA_EMBEDDING_MODEL)\n",
        "        _ = embedding_function.embed_query(\"test embedding\")\n",
        "        print(\"Ollama Embeddings initialized successfully.\")\n",
        "    except Exception as e:\n",
        "         print(f\"Error initializing Ollama Embeddings. Make sure Ollama is running and model '{OLLAMA_EMBEDDING_MODEL}' is pulled.\")\n",
        "         print(e)\n",
        "         return\n",
        "\n",
        "    if os.path.exists(CHROMA_DB_DIR):\n",
        "        print(f\"Loading existing Chroma DB from {CHROMA_DB_DIR}\")\n",
        "        try:\n",
        "            vectorstore = Chroma(persist_directory=CHROMA_DB_DIR, embedding_function=embedding_function)\n",
        "            print(\"Chroma DB loaded successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading Chroma DB: {e}. Rebuilding.\")\n",
        "            import shutil\n",
        "            shutil.rmtree(CHROMA_DB_DIR)\n",
        "            vectorstore = Chroma.from_documents(documents=split_documents, embedding=embedding_function, persist_directory=CHROMA_DB_DIR)\n",
        "            print(\"Chroma DB rebuilt.\")\n",
        "    else:\n",
        "        print(f\"Creating new Chroma DB at {CHROMA_DB_DIR}\")\n",
        "        vectorstore = Chroma.from_documents(documents=split_documents, embedding=embedding_function, persist_directory=CHROMA_DB_DIR)\n",
        "        print(\"Chroma DB created.\")\n",
        "\n",
        "    # --- 3. Setup RAG Chain with Memory ---\n",
        "    # (This part remains the same, using the vectorstore that now includes file data)\n",
        "    print(f\"Initializing Ollama LLM using model: {OLLAMA_LLM}\")\n",
        "    try:\n",
        "        ollama_llm = ChatOllama(\n",
        "            model=OLLAMA_LLM,\n",
        "            temperature=0.1\n",
        "            # base_url=\"http://localhost:11434\" # Uncomment if needed\n",
        "        )\n",
        "        print(\"Ollama LLM initialized successfully.\")\n",
        "        print(f\"Ensure Ollama server is running and model '{OLLAMA_LLM}' is pulled.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing Ollama LLM. Make sure Ollama server is running\")\n",
        "        print(f\"and model '{OLLAMA_LLM}' is pulled.\")\n",
        "        print(e)\n",
        "        return\n",
        "\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "    history_aware_retriever_prompt = ChatPromptTemplate.from_messages([\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        (\"user\", \"{input}\"),\n",
        "        (\"user\", \"Given the above conversation, generate a standalone question for search. Return only the question.\")\n",
        "    ])\n",
        "\n",
        "    history_aware_retriever_chain = create_history_aware_retriever(\n",
        "        ollama_llm,\n",
        "        retriever,\n",
        "        history_aware_retriever_prompt\n",
        "    )\n",
        "\n",
        "    rag_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"Answer the user's question based ONLY on the below context and the chat history.\n",
        "If you cannot find the answer in the provided context or chat history, state that you do not have enough information from the provided sources to answer the question. Do not make up an answer.\n",
        "\n",
        "Context: {context}\"\"\"),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        (\"user\", \"{input}\"),\n",
        "    ])\n",
        "\n",
        "    stuff_documents_chain = create_stuff_documents_chain(\n",
        "        ollama_llm,\n",
        "        rag_prompt\n",
        "    )\n",
        "\n",
        "    conversational_rag_chain = create_retrieval_chain(\n",
        "        history_aware_retriever_chain,\n",
        "        stuff_documents_chain\n",
        "    )\n",
        "\n",
        "    with_message_history = RunnableWithMessageHistory(\n",
        "        conversational_rag_chain,\n",
        "        get_session_history,\n",
        "        input_messages_key=\"input\",\n",
        "        history_messages_key=\"chat_history\",\n",
        "        output_messages_key=\"answer\", # Added in previous fix\n",
        "    )\n",
        "\n",
        "    print(\"\\nConversational RAG system (Ollama + Local Files) is ready. Ask me a question.\")\n",
        "    print(\"Type 'quit' to exit.\")\n",
        "    print(\"Sources include scraped websites and local .txt files.\")\n",
        "\n",
        "    # --- 4. Querying Loop ---\n",
        "    session_id = \"abc\"\n",
        "\n",
        "    while True:\n",
        "        question = input(\"\\nYour Question: \")\n",
        "        if question.lower() == 'quit':\n",
        "            break\n",
        "\n",
        "        if not question.strip():\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            response = with_message_history.invoke(\n",
        "                {\"input\": question},\n",
        "                config={\"configurable\": {\"session_id\": session_id}}\n",
        "            )\n",
        "\n",
        "            print(\"\\nAnswer:\")\n",
        "            print(response['answer'])\n",
        "\n",
        "            # Optional: Print sources (requires modifying the rag_prompt or chain)\n",
        "            # Currently, the retrieved 'context' is not directly in the final response dict\n",
        "            # You could modify the chain to return context explicitly if needed.\n",
        "            # print(\"\\nSources (based on chunks used):\")\n",
        "            # for doc in response.get('context', []): # If context were returned\n",
        "            #     print(f\"- {doc.metadata.get('source', 'Unknown Source')}\")\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during the RAG process: {e}\")\n",
        "            print(\"Please ensure Ollama server is running with the correct models pulled.\")\n",
        "            print(e)\n",
        "\n",
        "    print(\"Exiting.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-on 8\n",
    "#### In this hands-on, an LLM will be installed and served.\n",
    "#### Retrieval Augmented Generation (RAG) will be used to to produce more accurate and contextually relevant outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "uB3GaqzwmH72",
    "outputId": "d581c25d-3d7c-4bb2-d5c2-4f5c1770ecc5"
   },
   "outputs": [],
   "source": [
    "pip install requests beautifulsoup4 langchain langchain-community langchain-chroma langchain-ollama langchain-text-splitters colab-xterm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SR6YMNB-msY3"
   },
   "outputs": [],
   "source": [
    "!mkdir local_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "imSoxCuYh-vY",
    "outputId": "a78c2aba-c3b9-4269-dddf-8cb65f3e52c2"
   },
   "outputs": [],
   "source": [
    "!sudo apt update && sudo apt install pciutils lshw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R04AkQT2hBDl",
    "outputId": "c85833aa-caef-4b72-8245-b77da8c2b16d"
   },
   "outputs": [],
   "source": [
    "!curl https://ollama.ai/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N-YuIG1CmTfp"
   },
   "outputs": [],
   "source": [
    "!nohup ollama serve > ollama.log 2>&1 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RG2MCzu8h9mR",
    "outputId": "42fc3a9f-306e-4761-e75c-50f7e8c7fe7d"
   },
   "outputs": [],
   "source": [
    "!ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lTIqGMapjPsZ",
    "outputId": "de773234-f69b-4cb3-d138-cc204c3ea446"
   },
   "outputs": [],
   "source": [
    "!ollama pull gemma3:1b-it-qat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KrK2mSvmkJIF",
    "outputId": "278d90ea-258d-4f94-9062-7dc6121cc12a"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "# Import for loading local files\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "# Import Ollama specific components\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Import for Conversational Memory and Chains\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "# List of URLs to scrape. Replace with your desired websites.\n",
    "SOURCE_URLS = [\n",
    "    # Add more URLs here\n",
    "]\n",
    "\n",
    "# Local Directory for Text Files\n",
    "# IMPORTANT: Create this directory and place your .txt files inside\n",
    "LOCAL_DOCS_DIR = \"./local_docs\"\n",
    "\n",
    "# Ollama models\n",
    "OLLAMA_LLM = \"gemma3:1b-it-qat\" # Or another LLM you have pulled\n",
    "OLLAMA_EMBEDDING_MODEL = \"nomic-embed-text\" # Or another embedding model you have pulled\n",
    "\n",
    "# Directory for persistent vector store\n",
    "CHROMA_DB_DIR = \"./chroma_db_ollama_memory\"\n",
    "\n",
    "# In-memory storage for chat history (per session)\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> ChatMessageHistory:\n",
    "    \"\"\"Returns the chat message history for a given session ID.\"\"\"\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# --- Main RAG Implementation ---\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to perform RAG based on web and local text files using Ollama.\n",
    "    \"\"\"\n",
    "    # --- 1. Load Documents ---\n",
    "    print(\"Loading documents from URLs...\")\n",
    "    all_documents = []\n",
    "    # Load from URLs\n",
    "    for url in SOURCE_URLS:\n",
    "        try:\n",
    "            loader = WebBaseLoader(url)\n",
    "            docs = loader.load()\n",
    "            if docs:\n",
    "                print(f\"Loaded from web: {url}\")\n",
    "                all_documents.extend(docs)\n",
    "            else:\n",
    "                 print(f\"Could not load documents from: {url}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {url} using WebBaseLoader: {e}\")\n",
    "\n",
    "    # Load from local directory\n",
    "    print(f\"Loading documents from local directory: {LOCAL_DOCS_DIR}...\")\n",
    "    if os.path.exists(LOCAL_DOCS_DIR):\n",
    "        try:\n",
    "            # Use DirectoryLoader with TextLoader for .txt files\n",
    "            loader = DirectoryLoader(LOCAL_DOCS_DIR, glob=\"*.txt\", loader_cls=TextLoader)\n",
    "            local_docs = loader.load()\n",
    "            if local_docs:\n",
    "                print(f\"Loaded {len(local_docs)} documents from {LOCAL_DOCS_DIR}\")\n",
    "                all_documents.extend(local_docs)\n",
    "            else:\n",
    "                 print(f\"No .txt documents found in {LOCAL_DOCS_DIR}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading documents from local directory {LOCAL_DOCS_DIR}: {e}\")\n",
    "    else:\n",
    "        print(f\"Local documents directory not found: {LOCAL_DOCS_DIR}. Skipping local file loading.\")\n",
    "\n",
    "\n",
    "    if not all_documents:\n",
    "        print(\"No documents were loaded successfully from any source. Exiting.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Loaded {len(all_documents)} documents in total from all sources.\")\n",
    "\n",
    "    # --- 2. Process Documents (Splitting, Embedding, Vector Store) ---\n",
    "    # (This part processes the combined list of documents and remains the same)\n",
    "    print(\"Splitting documents into chunks...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    split_documents = text_splitter.split_documents(all_documents)\n",
    "    print(f\"Split into {len(split_documents)} chunks.\")\n",
    "\n",
    "    print(\"Initializing Ollama Embeddings...\")\n",
    "    try:\n",
    "        embedding_function = OllamaEmbeddings(model=OLLAMA_EMBEDDING_MODEL)\n",
    "        _ = embedding_function.embed_query(\"test embedding\")\n",
    "        print(\"Ollama Embeddings initialized successfully.\")\n",
    "    except Exception as e:\n",
    "         print(f\"Error initializing Ollama Embeddings. Make sure Ollama is running and model '{OLLAMA_EMBEDDING_MODEL}' is pulled.\")\n",
    "         print(e)\n",
    "         return\n",
    "\n",
    "    if os.path.exists(CHROMA_DB_DIR):\n",
    "        print(f\"Loading existing Chroma DB from {CHROMA_DB_DIR}\")\n",
    "        try:\n",
    "            vectorstore = Chroma(persist_directory=CHROMA_DB_DIR, embedding_function=embedding_function)\n",
    "            print(\"Chroma DB loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading Chroma DB: {e}. Rebuilding.\")\n",
    "            import shutil\n",
    "            shutil.rmtree(CHROMA_DB_DIR)\n",
    "            vectorstore = Chroma.from_documents(documents=split_documents, embedding=embedding_function, persist_directory=CHROMA_DB_DIR)\n",
    "            print(\"Chroma DB rebuilt.\")\n",
    "    else:\n",
    "        print(f\"Creating new Chroma DB at {CHROMA_DB_DIR}\")\n",
    "        vectorstore = Chroma.from_documents(documents=split_documents, embedding=embedding_function, persist_directory=CHROMA_DB_DIR)\n",
    "        print(\"Chroma DB created.\")\n",
    "\n",
    "    # --- 3. Setup RAG Chain with Memory ---\n",
    "    # (This part remains the same, using the vectorstore that now includes file data)\n",
    "    print(f\"Initializing Ollama LLM using model: {OLLAMA_LLM}\")\n",
    "    try:\n",
    "        ollama_llm = ChatOllama(\n",
    "            model=OLLAMA_LLM,\n",
    "            temperature=0.1\n",
    "            # base_url=\"http://localhost:11434\" # Uncomment if needed\n",
    "        )\n",
    "        print(\"Ollama LLM initialized successfully.\")\n",
    "        print(f\"Ensure Ollama server is running and model '{OLLAMA_LLM}' is pulled.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Ollama LLM. Make sure Ollama server is running\")\n",
    "        print(f\"and model '{OLLAMA_LLM}' is pulled.\")\n",
    "        print(e)\n",
    "        return\n",
    "\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "    history_aware_retriever_prompt = ChatPromptTemplate.from_messages([\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "        (\"user\", \"Given the above conversation, generate a standalone question for search. Return only the question.\")\n",
    "    ])\n",
    "\n",
    "    history_aware_retriever_chain = create_history_aware_retriever(\n",
    "        ollama_llm,\n",
    "        retriever,\n",
    "        history_aware_retriever_prompt\n",
    "    )\n",
    "\n",
    "    rag_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"Answer the user's question based ONLY on the below context and the chat history.\n",
    "If you cannot find the answer in the provided context or chat history, state that you do not have enough information from the provided sources to answer the question. Do not make up an answer.\n",
    "\n",
    "Context: {context}\"\"\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ])\n",
    "\n",
    "    stuff_documents_chain = create_stuff_documents_chain(\n",
    "        ollama_llm,\n",
    "        rag_prompt\n",
    "    )\n",
    "\n",
    "    conversational_rag_chain = create_retrieval_chain(\n",
    "        history_aware_retriever_chain,\n",
    "        stuff_documents_chain\n",
    "    )\n",
    "\n",
    "    with_message_history = RunnableWithMessageHistory(\n",
    "        conversational_rag_chain,\n",
    "        get_session_history,\n",
    "        input_messages_key=\"input\",\n",
    "        history_messages_key=\"chat_history\",\n",
    "        output_messages_key=\"answer\", # Added in previous fix\n",
    "    )\n",
    "\n",
    "    print(\"\\nConversational RAG system (Ollama + Local Files) is ready. Ask me a question.\")\n",
    "    print(\"Type 'quit' to exit.\")\n",
    "    print(\"Sources include scraped websites and local .txt files.\")\n",
    "\n",
    "    # --- 4. Querying Loop ---\n",
    "    session_id = \"abc\"\n",
    "\n",
    "    while True:\n",
    "        question = input(\"\\nYour Question: \")\n",
    "        if question.lower() == 'quit':\n",
    "            break\n",
    "\n",
    "        if not question.strip():\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            response = with_message_history.invoke(\n",
    "                {\"input\": question},\n",
    "                config={\"configurable\": {\"session_id\": session_id}}\n",
    "            )\n",
    "\n",
    "            print(\"\\nAnswer:\")\n",
    "            print(response['answer'])\n",
    "\n",
    "            # Optional: Print sources (requires modifying the rag_prompt or chain)\n",
    "            # Currently, the retrieved 'context' is not directly in the final response dict\n",
    "            # You could modify the chain to return context explicitly if needed.\n",
    "            # print(\"\\nSources (based on chunks used):\")\n",
    "            # for doc in response.get('context', []): # If context were returned\n",
    "            #     print(f\"- {doc.metadata.get('source', 'Unknown Source')}\")\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during the RAG process: {e}\")\n",
    "            print(\"Please ensure Ollama server is running with the correct models pulled.\")\n",
    "            print(e)\n",
    "\n",
    "    print(\"Exiting.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
